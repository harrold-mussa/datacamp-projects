### Data Engineering Project Portfolio (DataCamp Pro)

**Objective:** Built a robust data engineering project portfolio to solidify practical skills and experience across the modern data stack, focusing on ingestion, transformation, orchestration, and cloud infrastructure.

**Core Technical Skills Demonstrated:**

* **SQL & Data Modeling:** Designed and optimized relational schemas and performed complex data analysis, focusing on window functions, common table expressions (CTEs), and performance tuning for large datasets.
* **Python (Pandas, NumPy, Built-in Modules):** Developed efficient data processing and cleaning scripts. Leveraged **Pandas** for high-performance data manipulation (e.g., ETL), **NumPy** for vectorized operations, and native Python built-in modules for file handling, logging, and application logic.
* **Workflow Orchestration (Apache Airflow):** Created and managed production-ready **Directed Acyclic Graphs (DAGs)** to schedule, automate, and monitor complex data pipelines, ensuring data freshness and recovery through task dependencies and logging.
* **Cloud Data Warehousing (Snowflake & Databricks):**
    * **Snowflake:** Experienced in loading structured and semi-structured data, optimizing virtual warehouses, and utilizing features like Zero-Copy Cloning and Time Travel for efficient data management.
    * **Databricks:** Used **Spark SQL** and **PySpark** within Databricks notebooks for distributed data processing and scaled transformations of massive datasets, often within a **Delta Lake** architecture.
* **Cloud Infrastructure & Deployment (AWS & GCP):** Applied fundamental knowledge of cloud services for data storage (e.g., S3/GCS), compute (e.g., EC2/Cloud Compute), and integration points for data pipeline deployment on both **AWS** and **GCP** environments.
* **Version Control (Git):** Proficiently managed project source code using **Git** and GitHub, including branching, merging, and resolving conflicts to facilitate collaborative and maintainable data engineering workflows.

**Project Highlights (Examples):**

* **ETL Pipeline Construction:** Engineered an end-to-end data pipeline using **Apache Airflow** to extract data from an API source, transform it using **Pandas** and **SQL**, and load the clean, final data into **Snowflake** for downstream consumption.
* **Data Lake Processing:** Implemented a scalable transformation job in **Databricks (PySpark)** to process petabyte-scale semi-structured data, ultimately cleaning and structuring the data for BI reporting.